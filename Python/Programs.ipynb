{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848190b",
   "metadata": {},
   "source": [
    "**Remove punctuation and stop words from a paragraph using nltk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9791a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90db9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e6bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'paragraph', 'contains', 'punctuation', 'stop', 'words', 'like', \"'is\", \"'the\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"This is a sample paragraph. It contains punctuation, and stop words like 'is', 'a', and 'the'.\"\n",
    "\n",
    "# Tokenize the paragraph into words\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "# Convert to lowercase\n",
    "words = [word.lower() for word in words]\n",
    "\n",
    "# Remove punctuation\n",
    "words = [word for word in words if word not in string.punctuation]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Output result\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76899f57",
   "metadata": {},
   "source": [
    "**Perform stemming and lemmatization on user-input text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430dc190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For lemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')  # Optional: for better lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9003be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: This is a Dr SAket class\n",
      "\n",
      "Word\tStem\tLemma\n",
      "------------------------------\n",
      "This\tthi\tThis\n",
      "is\tis\tbe\n",
      "a\ta\ta\n",
      "Dr\tdr\tDr\n",
      "SAket\tsaket\tSAket\n",
      "class\tclass\tclass\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Get user input\n",
    "text = input(\"Enter a sentence: \")\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Perform stemming and lemmatization\n",
    "print(\"\\nWord\\tStem\\tLemma\")\n",
    "print(\"-\" * 30)\n",
    "for word, tag in pos_tags:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "    print(f\"{word}\\t{stem}\\t{lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5435ef",
   "metadata": {},
   "source": [
    "**Apply POS (Part of Speech) tagging using NLTK on a given sentence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92035098",
   "metadata": {},
   "source": [
    "**Common POS Tags (Short Guide)**  \n",
    "Tag         Meaning    \n",
    "NN  \tNoun, singular  \n",
    "NNS \tNoun, plural  \n",
    "VB  \tVerb, base form  \n",
    "VBZ\t    Verb, 3rd person  \n",
    "VBD\t    Verb, past tense  \n",
    "JJ\t    Adjective  \n",
    "RB\t    Adverb  \n",
    "DT\t    Determiner  \n",
    "IN\t    Preposition  \n",
    "PRP\t    Personal pronoun  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: This is Artificial INtelligence Lab\n",
      "\n",
      "POS Tags:\n",
      "This --> DT\n",
      "is --> VBZ\n",
      "Artificial --> JJ\n",
      "INtelligence --> NNP\n",
      "Lab --> NNP\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Input sentence\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Output\n",
    "print(\"\\nPOS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word} --> {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4de5aa",
   "metadata": {},
   "source": [
    "**Build a simple text classifier using NLTK e.g. classify messages as spam / ham**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef96377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Free money!!!\", \"spam\"),\n",
    "    (\"Hi, how are you?\", \"ham\"),\n",
    "    (\"Win a free iPhone now\", \"spam\"),\n",
    "    (\"Are we still meeting tomorrow?\", \"ham\"),\n",
    "    (\"Limited offer! Call now!\", \"spam\"),\n",
    "    (\"Don't forget our lunch meeting\", \"ham\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b030bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Most Informative Features\n",
      "                       ! = None              ham : spam   =      1.7 : 1.0\n",
      "                       , = None             spam : ham    =      1.7 : 1.0\n",
      "                       a = None              ham : spam   =      1.7 : 1.0\n",
      "                      hi = None             spam : ham    =      1.7 : 1.0\n",
      "                     how = None             spam : ham    =      1.7 : 1.0\n",
      "                  iphone = None              ham : spam   =      1.7 : 1.0\n",
      "                 meeting = None             spam : ham    =      1.7 : 1.0\n",
      "                   money = None              ham : spam   =      1.7 : 1.0\n",
      "                     now = None              ham : spam   =      1.7 : 1.0\n",
      "                   still = None             spam : ham    =      1.7 : 1.0\n",
      "Prediction: spam\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "\n",
    "# Create feature extractor\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Apply feature extractor\n",
    "feature_set = [(extract_features(text), label) for (text, label) in data]\n",
    "\n",
    "# Split into train and test\n",
    "train_set = feature_set[:4]\n",
    "test_set = feature_set[4:]\n",
    "\n",
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(\"Accuracy:\", accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "\n",
    "msg = \"Congratulations! You have won a free ticket\"\n",
    "features = extract_features(msg)\n",
    "print(\"Prediction:\", classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4826285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
